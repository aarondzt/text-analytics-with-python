{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('C:/Users/HP/Documents/NLP/slatestarcodex.year.2020.txt', 'r', encoding = 'utf-8')\n",
    "posts2020 = f.read()\n",
    "f = open('C:/Users/HP/Documents/NLP/slatestarcodex.year.2019.txt', 'r', encoding = 'utf-8')\n",
    "posts2019 = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts2020 = posts2020.split('\\n\\n\\n\\n\\n\\n')\n",
    "posts2019 = posts2019.split('\\n\\n\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = posts2020 + posts2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, post in enumerate(posts):\n",
    "#     print('==========' + str(i) + '==========')\n",
    "#     print(post[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "f = open('C:/Users/HP/Documents/NLP/MySQL_stopwords.txt', 'r', encoding = 'utf-8')\n",
    "stop_words = f.read()\n",
    "stop_words = re.split(' \\t|\\n', stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Text Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "from nltk import TweetTokenizer\n",
    "# stop_words = nltk.corpus.stopwords.words('english')\n",
    "tokenizer = TweetTokenizer()\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "def normalize_corpora(corpora):\n",
    "    normalized_corpora = []\n",
    "    for corpus in corpora:\n",
    "        # Lowercase\n",
    "        corpus = corpus.lower()\n",
    "        # Replace \n",
    "        corpus = corpus.replace(\"/\", \" \")\n",
    "        corpus = corpus.replace(\"â€™\", \"'\")\n",
    "        corpus = corpus.replace(\"'s\", \"\")\n",
    "        # Remove numbers\n",
    "        corpus = re.sub('[^A-Za-z-\\']+', ' ', corpus)\n",
    "        # Strip spaces\n",
    "        corpus_tokens = tokenizer.tokenize(corpus)\n",
    "        # Remove stopwords\n",
    "        corpus_tokens = [token for token in corpus_tokens if token not in stop_words]\n",
    "        # Lemmatize\n",
    "        corpus_tokens = [lemmatizer.lemmatize(token) for token in corpus_tokens if not token.isnumeric()]\n",
    "        # Remove single characters\n",
    "        corpus_tokens = [token for token in corpus_tokens if len(token) > 1]\n",
    "        # Remove empty corpus\n",
    "        if corpus_tokens:\n",
    "            normalized_corpora.append(corpus_tokens)\n",
    "    return normalized_corpora\n",
    "normalized_posts = normalize_corpora(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_no_none = []\n",
    "for post in posts:\n",
    "    post = post.replace(\"\\n\", \" \")\n",
    "    if post:\n",
    "        posts_no_none.append(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Representation with Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "bigram = gensim.models.Phrases(normalized_posts, min_count = 10, threshold = 10, delimiter = b'_')\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doctor',\n",
       " 'ninety',\n",
       " 'percent',\n",
       " 'driver',\n",
       " 'above-average',\n",
       " 'driver',\n",
       " 'ninety',\n",
       " 'percent',\n",
       " 'professor',\n",
       " 'above-average',\n",
       " 'professor',\n",
       " 'relevant',\n",
       " 'study',\n",
       " 'paywalled',\n",
       " 'trust',\n",
       " 'recent',\n",
       " 'discussion',\n",
       " 'therapy_book',\n",
       " 'make_sense',\n",
       " 'ninety',\n",
       " 'percent',\n",
       " 'therapist',\n",
       " 'believed',\n",
       " 'above-average',\n",
       " 'therapist',\n",
       " 'pretty',\n",
       " 'ninety',\n",
       " 'percent',\n",
       " 'doctor',\n",
       " 'above-average',\n",
       " 'doctor',\n",
       " 'trap',\n",
       " 'noticed',\n",
       " 'falling',\n",
       " 'explain',\n",
       " 'patient',\n",
       " 'doctor',\n",
       " 'worse',\n",
       " 'good',\n",
       " 'doctor',\n",
       " 'stay',\n",
       " 'patient',\n",
       " 'bad',\n",
       " 'doctor',\n",
       " 'doctor',\n",
       " 'doctor',\n",
       " 'current',\n",
       " 'patient',\n",
       " 'doctor',\n",
       " 'worse']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model[normalized_posts[0]][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample word to number mappings: [(0, 'above-average'), (1, 'absolutely'), (2, 'addiction'), (3, 'addictive'), (4, 'address'), (5, 'afford'), (6, 'annoying'), (7, 'answer'), (8, 'appointment'), (9, 'assume'), (10, 'autonomy'), (11, 'average'), (12, 'avoid'), (13, 'awkward'), (14, 'bad')]\n",
      "Total Vocabulary Size: 22520\n"
     ]
    }
   ],
   "source": [
    "normalized_corpus_bigrams = [bigram_model[post] for post in normalized_posts]\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = gensim.corpora.Dictionary(normalized_corpus_bigrams)\n",
    "print('Sample word to number mappings:', list(dictionary.items())[:15])\n",
    "print('Total Vocabulary Size:', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size: 4706\n"
     ]
    }
   ],
   "source": [
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below = 5, no_above = 0.5)\n",
    "print('Total Vocabulary Size:', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 2), (10, 1), (11, 1), (12, 1), (13, 2), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 5), (21, 1), (22, 1), (23, 6), (24, 1), (25, 1), (26, 1), (27, 1), (28, 2), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 3), (44, 1), (45, 1), (46, 1), (47, 1), (48, 2), (49, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Transforming corpus into bag of words vectors\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in normalized_corpus_bigrams]\n",
    "print(bow_corpus[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('benefit', 2), ('care', 1), ('focused', 1), ('forget', 1), ('hard', 1), ('left', 1), ('month', 3), ('place', 1), ('practice', 1), ('raise', 1), ('relevant', 1), ('state', 1), ('transfer', 1), ('true', 1), ('worse', 1), ('abuse', 1), ('accusation', 1), ('accused', 1), ('achieved', 1), ('action', 2), ('admit', 1), ('africa', 1), ('agreed', 1), ('algorithm', 1), ('american', 1), ('ancestor', 1), ('ancient', 1), ('argued', 1), ('attention', 1), ('back', 1), ('background', 2), ('ban', 1), ('based', 1), ('basic_income', 1), ('belief', 1), ('bend', 2), ('biden', 2), ('biology', 1), ('bird', 5), ('board', 1), ('building', 1), ('called', 1), ('campaign', 4), ('child', 1), ('chosen', 1), ('citizen', 1), ('city', 2), ('claim', 3), ('close', 1), ('coast', 1)]\n"
     ]
    }
   ],
   "source": [
    "print([(dictionary[idx] , freq) for idx, freq in bow_corpus[1][:50]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Models with Latent Semantic Indexing (LSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 38.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TOTAL_TOPICS = 10\n",
    "lsi_bow = gensim.models.LsiModel(bow_corpus, id2word = dictionary, num_topics = TOTAL_TOPICS,\n",
    "                                 onepass = True, chunksize = len(bow_corpus), power_iters = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for topic_id, topic in lsi_bow.print_topics(num_topics = TOTAL_TOPICS, num_words = 10):\n",
    "#     print('Topic #' + str(topic_id+1) + ':')\n",
    "#     print(topic)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "==================================================\n",
      "Direction 1: []\n",
      "--------------------------------------------------\n",
      "Direction 2: [('study', -0.161), ('human', -0.147), ('college', -0.139), ('experience', -0.133), ('life', -0.128), ('case', -0.113), ('number', -0.111), ('animal', -0.105), ('book', -0.096), ('government', -0.09), ('bad', -0.088), ('idea', -0.088), ('student', -0.085), ('important', -0.084), ('true', -0.083), ('evidence', -0.081), ('system', -0.079), ('level', -0.079), ('reason', -0.078), ('state', -0.078)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #2:\n",
      "==================================================\n",
      "Direction 1: [('experience', 0.176), ('human', 0.125), ('animal', 0.115), ('study', 0.099), ('life', 0.087)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('college', -0.608), ('student', -0.294), ('harvard', -0.253), ('school', -0.203), ('number', -0.124), ('admission', -0.113), ('applicant', -0.11), ('increasing', -0.11), ('lawyer', -0.088), ('top', -0.083), ('application', -0.08), ('education', -0.064), ('tuition', -0.064), ('increased', -0.063), ('medical_school', -0.062)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #3:\n",
      "==================================================\n",
      "Direction 1: [('experience', 0.312), ('college', 0.208), ('fetus', 0.162), ('study', 0.161), ('abortion', 0.157), ('human', 0.137), ('animal', 0.124), ('perception', 0.111), ('student', 0.097), ('diet', 0.094), ('harvard', 0.092)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('contact', -0.21), ('location', -0.198), ('government', -0.189), ('billionaire', -0.158), ('money', -0.13), ('book', -0.1), ('million', -0.094), ('spending', -0.086), ('power', -0.079)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #4:\n",
      "==================================================\n",
      "Direction 1: [('billionaire', 0.081), ('study', 0.078), ('government', 0.076), ('gene', 0.073)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('location', -0.504), ('contact', -0.487), ('experience', -0.261), ('detail', -0.185), ('dot', -0.17), ('gmail', -0.136), ('fetus', -0.104), ('october_pm', -0.102), ('abortion', -0.102), ('meetup', -0.101), ('perception', -0.096), ('belief', -0.087), ('college', -0.085), ('st', -0.083), ('city', -0.082), ('god', -0.073)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #5:\n",
      "==================================================\n",
      "Direction 1: [('study', 0.235), ('animal', 0.234), ('contact', 0.226), ('location', 0.221), ('diet', 0.169), ('gene', 0.155), ('human', 0.127), ('cell', 0.127), ('food', 0.127), ('disease', 0.127), ('chicken', 0.083), ('obesity', 0.081)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('experience', -0.357), ('belief', -0.156), ('god', -0.148), ('perception', -0.13), ('argument', -0.109), ('book', -0.103), ('spiritual_experience', -0.102), ('paradigm', -0.084)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #6:\n",
      "==================================================\n",
      "Direction 1: [('paradigm', 0.174), ('theory', 0.139), ('science', 0.109), ('kuhn', 0.101), ('study', 0.094)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('fetus', -0.316), ('abortion', -0.308), ('billionaire', -0.252), ('government', -0.207), ('future', -0.169), ('money', -0.158), ('life', -0.14), ('spending', -0.122), ('woman', -0.118), ('charity', -0.108), ('animal', -0.108), ('billionaire_philanthropy', -0.101), ('pregnancy', -0.099), ('mother', -0.099), ('foundation', -0.092)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #7:\n",
      "==================================================\n",
      "Direction 1: [('fetus', 0.266), ('abortion', 0.259), ('study', 0.175), ('gene', 0.158), ('future', 0.144), ('woman', 0.142), ('disease', 0.118), ('risk', 0.116), ('patient', 0.113), ('case', 0.09)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('animal', -0.347), ('experience', -0.19), ('chicken', -0.172), ('pig', -0.134), ('billionaire', -0.11), ('meat', -0.103), ('life', -0.102), ('cow', -0.096), ('human', -0.096), ('consciousness', -0.092)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #8:\n",
      "==================================================\n",
      "Direction 1: [('animal', 0.153), ('fetus', 0.149), ('abortion', 0.145), ('future', 0.113), ('model', 0.104), ('paradigm', 0.103), ('chicken', 0.089)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('study', -0.277), ('billionaire', -0.244), ('experience', -0.198), ('gene', -0.192), ('government', -0.166), ('disease', -0.146), ('money', -0.122), ('spending', -0.107), ('charity', -0.098), ('billionaire_philanthropy', -0.097), ('foundation', -0.093), ('effect', -0.092), ('spiritual_experience', -0.089)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #9:\n",
      "==================================================\n",
      "Direction 1: [('gene', 0.269), ('cell', 0.23), ('competition', 0.188), ('population', 0.14), ('genetic', 0.14), ('cancer', 0.135), ('slack', 0.122), ('paradigm', 0.115), ('disease', 0.114), ('human', 0.102), ('mutation', 0.1), ('individual', 0.097), ('technology', 0.093), ('evolutionary', 0.091)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('doctor', -0.197), ('patient', -0.19), ('study', -0.182), ('pain', -0.143), ('drug', -0.138), ('addiction', -0.128)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #10:\n",
      "==================================================\n",
      "Direction 1: [('wage', 0.227), ('worker', 0.189), ('productivity', 0.187), ('inequality', 0.153), ('cycle', 0.15), ('population', 0.143), ('period', 0.134), ('experience', 0.122), ('study', 0.114), ('inflation', 0.112), ('labor', 0.112), ('graph', 0.107), ('trend', 0.106), ('increasing', 0.102)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('paradigm', -0.239), ('gene', -0.163), ('kuhn', -0.137), ('science', -0.124), ('patient', -0.11), ('college', -0.099)]\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(TOTAL_TOPICS):\n",
    "    print('Topic #'+str(n+1)+':')\n",
    "    print('=' * 50)\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    for term, wt in lsi_bow.show_topic(n, topn = 20):\n",
    "        if wt >= 0:\n",
    "            d1.append((term, round(wt, 3)))\n",
    "        else:\n",
    "            d2.append((term, round(wt, 3)))\n",
    "    print('Direction 1:', d1)\n",
    "    print('-'*50)\n",
    "    print('Direction 2:', d2)\n",
    "    print('-'*50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4706, 10), (10,), (10, 247))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_topic = lsi_bow.projection.u\n",
    "singular_values = lsi_bow.projection.s\n",
    "topic_document = (gensim.matutils.corpus2dense(lsi_bow[bow_corpus], len(singular_values)).T / singular_values).T\n",
    "term_topic.shape, singular_values.shape, topic_document.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "document_topics = pd.DataFrame(np.round(topic_document.T, 3), \n",
    "                               columns=['T' + str(i) for i in range(1, TOTAL_TOPICS+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document #50:\n",
      "Dominant Topics (top 3): ['T1', 'T5', 'T6']\n",
      "Paper Summary:\n",
      "Book Review: Origin Of Consciousness In The Breakdown Of The Bicameral Mind\n",
      "\n",
      "\n",
      "I.\n",
      "\n",
      "Julian Jaynesâ€™ The Origin Of Consciousness In The Breakdown Of The Bicameral Mind is a brilliant book, with only two minor flaws. First, that it purports to explains the origin of consciousness. And second, that it posits a breakdown of the bicameral mind. I think itâ€™s possible to route around these flaws while keeping the thesis otherwise intact. So Iâ€™m going to start by reviewing a slightly different book, the on\n",
      "\n",
      "Document #100:\n",
      "Dominant Topics (top 3): ['T7', 'T8', 'T5']\n",
      "Paper Summary:\n",
      "OT124: Opentatonic Thread\n",
      "\n",
      "\n",
      "This is the bi-weekly visible open thread (there are also hidden open threads twice a week you can reach through the Open Thread tab on the top of the page). Post about anything you want, but please try to avoid hot-button political and social topics. You can also talk at the SSC subreddit or the SSC Discord server â€“ and also check out the SSC Podcast. Also:\n",
      "\n",
      "1. The DC SSC meetup group is celebrating its second anniversary and wants to throw a party. Itâ€™s Saturday Apr\n",
      "\n",
      "Document #150:\n",
      "Dominant Topics (top 3): ['T1', 'T8', 'T2']\n",
      "Paper Summary:\n",
      "Highlights From The Comments On Cultural Evolution\n",
      "\n",
      "\n",
      "Peter Gerdes says:\n",
      "\n",
      "As the examples of the Nicaraguan deaf children left on their own to develop their own language demonstrates (as do other examples) we do create languages very very quickly in a social environment.\n",
      "\n",
      "Creating conlangs is hard not because creating language is fundamentally hard but because we are bad at top down modelling of processes that are the result of a bunch of tiny modifications over time. The distinctive features of \n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_numbers = [50, 100, 150]\n",
    "for document_number in document_numbers:\n",
    "    top_topics = list(document_topics.columns[np.argsort(-np.absolute(document_topics.iloc[document_number].values))[:3]])\n",
    "    print('Document #' + str(document_number) + ':')\n",
    "    print('Dominant Topics (top 3):', top_topics)\n",
    "    print('Paper Summary:')\n",
    "    print(posts[document_number][:500])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Models with Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda_model = gensim.models.LdaModel(corpus = bow_corpus, id2word = dictionary, chunksize = len(bow_corpus), \n",
    "                                   alpha = 'auto', eta = 'auto', random_state = 42,\n",
    "                                   iterations = 500, num_topics = TOTAL_TOPICS, \n",
    "                                   passes = 50, eval_every = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Coherence Score: -1.9371252861506711\n"
     ]
    }
   ],
   "source": [
    "topics_coherences = lda_model.top_topics(bow_corpus, topn=20)\n",
    "avg_coherence_score = np.mean([item[1] for item in topics_coherences])\n",
    "print('Avg. Coherence Score:', avg_coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics with Weights\n",
      "==================================================\n",
      "Topic #1:\n",
      "[('patient', 0.007), ('theory', 0.006), ('doctor', 0.005), ('study', 0.004), ('book', 0.004), ('evidence', 0.004), ('human', 0.004), ('mask', 0.004), ('case', 0.004), ('true', 0.003), ('process', 0.003), ('back', 0.003), ('real', 0.003), ('bad', 0.003), ('pain', 0.003), ('culture', 0.003), ('state', 0.003), ('feel', 0.003), ('science', 0.003), ('level', 0.003)]\n",
      "\n",
      "Topic #2:\n",
      "[('idea', 0.008), ('group', 0.006), ('bias', 0.006), ('competition', 0.006), ('child', 0.005), ('survey', 0.005), ('book', 0.005), ('society', 0.005), ('company', 0.005), ('result', 0.004), ('argument', 0.004), ('effect', 0.004), ('question', 0.004), ('system', 0.003), ('woman', 0.003), ('animal', 0.003), ('person', 0.003), ('evolution', 0.003), ('social', 0.003), ('bad', 0.003)]\n",
      "\n",
      "Topic #3:\n",
      "[('government', 0.01), ('money', 0.007), ('country', 0.006), ('article', 0.006), ('coronavirus', 0.005), ('million', 0.005), ('billionaire', 0.005), ('case', 0.005), ('life', 0.004), ('charity', 0.004), ('tax', 0.004), ('bad', 0.003), ('system', 0.003), ('spending', 0.003), ('american', 0.003), ('number', 0.003), ('public', 0.003), ('plan', 0.003), ('state', 0.003), ('cost', 0.003)]\n",
      "\n",
      "Topic #4:\n",
      "[('study', 0.027), ('effect', 0.012), ('gene', 0.011), ('depression', 0.008), ('patient', 0.007), ('disease', 0.007), ('result', 0.007), ('risk', 0.007), ('diet', 0.007), ('human', 0.006), ('cell', 0.006), ('ssri', 0.006), ('antidepressant', 0.005), ('rate', 0.005), ('genetic', 0.004), ('cancer', 0.004), ('higher', 0.004), ('food', 0.004), ('group', 0.004), ('evidence', 0.004)]\n",
      "\n",
      "Topic #5:\n",
      "[('animal', 0.014), ('human', 0.011), ('abortion', 0.008), ('fetus', 0.008), ('life', 0.007), ('future', 0.007), ('chicken', 0.006), ('population', 0.006), ('rate', 0.005), ('experience', 0.005), ('meat', 0.005), ('conscious', 0.005), ('number', 0.004), ('brain', 0.004), ('harm', 0.004), ('woman', 0.004), ('argument', 0.004), ('result', 0.004), ('growth', 0.004), ('consciousness', 0.004)]\n",
      "\n",
      "Topic #6:\n",
      "[('paradigm', 0.006), ('day', 0.004), ('back', 0.004), ('made', 0.004), ('prediction', 0.004), ('company', 0.004), ('drug', 0.004), ('theory', 0.004), ('science', 0.004), ('fda', 0.003), ('wrong', 0.003), ('kuhn', 0.003), ('started', 0.003), ('true', 0.003), ('system', 0.003), ('bad', 0.003), ('thought', 0.003), ('scientific', 0.003), ('great', 0.003), ('earth', 0.003)]\n",
      "\n",
      "Topic #7:\n",
      "[('college', 0.031), ('student', 0.021), ('school', 0.019), ('harvard', 0.012), ('number', 0.008), ('education', 0.006), ('top', 0.006), ('class', 0.005), ('university', 0.005), ('applicant', 0.005), ('application', 0.005), ('admission', 0.005), ('past', 0.004), ('increase', 0.004), ('therapy', 0.004), ('increasing', 0.004), ('doctor', 0.004), ('read', 0.004), ('increased', 0.004), ('rate', 0.004)]\n",
      "\n",
      "Topic #8:\n",
      "[('ai', 0.005), ('gpt', 0.005), ('human', 0.005), ('job', 0.005), ('model', 0.004), ('wage', 0.004), ('word', 0.004), ('book', 0.004), ('period', 0.004), ('number', 0.004), ('worker', 0.003), ('cycle', 0.003), ('thread', 0.003), ('population', 0.003), ('post', 0.003), ('data', 0.003), ('cost', 0.003), ('productivity', 0.003), ('level', 0.003), ('feel', 0.003)]\n",
      "\n",
      "Topic #9:\n",
      "[('experience', 0.025), ('contact', 0.011), ('location', 0.01), ('belief', 0.01), ('god', 0.008), ('perception', 0.008), ('spiritual_experience', 0.007), ('participant', 0.006), ('meetup', 0.006), ('psychedelics', 0.005), ('autism', 0.005), ('detail', 0.005), ('group', 0.004), ('prior', 0.004), ('evidence', 0.004), ('city', 0.004), ('dot', 0.004), ('view', 0.004), ('sense', 0.004), ('religious', 0.004)]\n",
      "\n",
      "Topic #10:\n",
      "[('couple', 0.012), ('marriage', 0.012), ('partner', 0.01), ('open_thread', 0.009), ('wife', 0.008), ('comment', 0.008), ('relationship', 0.007), ('corn', 0.006), ('smell', 0.006), ('conflict', 0.006), ('husband', 0.006), ('study', 0.005), ('love', 0.005), ('ot', 0.005), ('blog', 0.004), ('rule', 0.004), ('therapist', 0.004), ('feel', 0.004), ('divorce', 0.004), ('ssc_subreddit', 0.004)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics_with_wts = [item[0] for item in topics_coherences]\n",
    "print('LDA Topics with Weights')\n",
    "print('='*50)\n",
    "for idx, topic in enumerate(topics_with_wts):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print([(term, round(wt, 3)) for wt, term in topic])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Coherence Score (Cv): 0.3416720490885158\n",
      "Avg. Coherence Score (UMass): -1.9371252861506716\n",
      "Model Perplexity: -7.826103585722698\n"
     ]
    }
   ],
   "source": [
    "cv_coherence_model_lda = gensim.models.CoherenceModel(model = lda_model, corpus = bow_corpus, \n",
    "                                                      texts = normalized_corpus_bigrams,\n",
    "                                                      dictionary = dictionary, \n",
    "                                                      coherence = 'c_v')\n",
    "avg_coherence_cv = cv_coherence_model_lda.get_coherence()\n",
    "umass_coherence_model_lda = gensim.models.CoherenceModel(model = lda_model, corpus = bow_corpus, \n",
    "                                                         texts = normalized_corpus_bigrams,\n",
    "                                                         dictionary = dictionary, \n",
    "                                                         coherence = 'u_mass')\n",
    "avg_coherence_umass = umass_coherence_model_lda.get_coherence()\n",
    "perplexity = lda_model.log_perplexity(bow_corpus)\n",
    "print('Avg. Coherence Score (Cv):', avg_coherence_cv)\n",
    "print('Avg. Coherence Score (UMass):', avg_coherence_umass)\n",
    "print('Model Perplexity:', perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Tuning - Finding Optimal Number of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MALLET_PATH = 'C:/mallet-2.0.8/bin/mallet'\n",
    "import os\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from tqdm import tqdm\n",
    "os.environ['MALLET_HOME'] = 'C:/mallet-2.0.8'\n",
    "\n",
    "def topic_model_coherence_generator(corpus, texts, dictionary, \n",
    "                                    start_topic_count = 1, end_topic_count = 20, step = 1,\n",
    "                                    cpus = 8):\n",
    "    models = []\n",
    "    coherence_scores = []\n",
    "    for topic_nums in tqdm(range(start_topic_count, end_topic_count + 1, step)):\n",
    "        mallet_lda_model = gensim.models.wrappers.LdaMallet(mallet_path = MALLET_PATH, corpus = corpus,\n",
    "                                                            num_topics = topic_nums, id2word = dictionary,\n",
    "                                                            iterations = 100, workers = cpus, random_seed = 20210224)\n",
    "        cv_coherence_model_mallet_lda = gensim.models.CoherenceModel(model = mallet_lda_model, corpus = corpus, \n",
    "                                                                     texts = texts, dictionary = dictionary, \n",
    "                                                                     coherence = 'c_v')\n",
    "        coherence_score = cv_coherence_model_mallet_lda.get_coherence()\n",
    "        coherence_scores.append(coherence_score)\n",
    "        models.append(mallet_lda_model)\n",
    "    return models, coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                 | 16/40 [05:13<08:18, 20.79s/it]"
     ]
    }
   ],
   "source": [
    "end_topic_count = 40\n",
    "lda_models, coherence_scores = topic_model_coherence_generator(corpus = bow_corpus, texts = normalized_corpus_bigrams,\n",
    "                                                               dictionary = dictionary, start_topic_count = 1,\n",
    "                                                               end_topic_count = end_topic_count, step = 1, cpus = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_df = pd.DataFrame({'Number of Topics': range(1, end_topic_count + 1, 1),\n",
    "                             'Coherence Score': np.round(coherence_scores, 4)})\n",
    "coherence_df = coherence_df.sort_values(by = 'Coherence Score', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "x_ax = range(1, end_topic_count + 1, 1)\n",
    "y_ax = coherence_scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_ax, y_ax, c = 'r')\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "xl = plt.xlabel('Number of Topics')\n",
    "yl = plt.ylabel('Coherence Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_idx = coherence_df['Number of Topics'].index[0]\n",
    "best_lda_model = lda_models[best_model_idx]\n",
    "best_lda_model.num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [[(term, round(wt, 3)) \n",
    "               for term, wt in best_lda_model.show_topic(n, topn=20)] \n",
    "                   for n in range(0, best_lda_model.num_topics)]\n",
    "for idx, topic in enumerate(topics):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print([term for term, wt in topic])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "topics_df = pd.DataFrame([', '.join([term for term, wt in topic])  \n",
    "                              for topic in topics],\n",
    "                         columns = ['Topic Desc'],\n",
    "                         index = range(1, best_lda_model.num_topics + 1)\n",
    "                         )\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Topic Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_results = best_lda_model[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_topics = [sorted(topics, key = lambda record: -record[1])[0] for topics in tm_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_topic_df = pd.DataFrame()\n",
    "corpus_topic_df['Document'] = range(0, len(posts_no_none))\n",
    "corpus_topic_df['Dominant Topic'] = [item[0] + 1 for item in corpus_topics]\n",
    "corpus_topic_df['Contribution %'] = [round(item[1] * 100, 2) for item in corpus_topics]\n",
    "corpus_topic_df['Topic Desc'] = [topics_df.iloc[t[0]]['Topic Desc'] for t in corpus_topics]\n",
    "corpus_topic_df['Post'] = posts_no_none\n",
    "corpus_topic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dominant Topics Distribution across Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_stats_df = corpus_topic_df.groupby('Dominant Topic').count()\n",
    "topic_stats_df = topic_stats_df.drop(['Contribution %', 'Topic Desc', 'Post'], axis = 1)\n",
    "topic_stats_df.columns = ['# of Docs']\n",
    "topic_stats_df['% Total Docs'] = round(100 * topic_stats_df['# of Docs'] / sum(topic_stats_df['# of Docs']), 2)\n",
    "topic_stats_df['Topic Desc'] = topics_df['Topic Desc']\n",
    "topic_stats_df.sort_values('% Total Docs', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant Posts per Topic based on Dominance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_posts = corpus_topic_df.groupby('Dominant Topic') \\\n",
    ".apply(lambda topic_set: (topic_set.sort_values(by=['Contribution %'], ascending=False).iloc[0]))\n",
    "relevant_posts.sort_values('Contribution %', ascending = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
