{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "f = open('C:/Users/HP/Documents/NLP/Ezo.txt', 'r', encoding = 'utf-8')\n",
    "corpus = f.read()\n",
    "corpus = corpus.replace(\"\\n\\n\", \" \")\n",
    "corpus = corpus.replace(\"â€”\", \" \")\n",
    "corpus = corpus.replace(\";\", \".\")\n",
    "import re\n",
    "corpus = re.sub('[^A-Za-z.]+', ' ', corpus)\n",
    "corpus = corpus.replace(\" . \", \". \")\n",
    "corpus = corpus[:-1]\n",
    "corpus_unpreprocessed = corpus\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = nltk.sent_tokenize(corpus)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "    \n",
    "def lemmatize_sentence(sentence):\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "lemmatized_sentences = []\n",
    "for sentence in sentences:\n",
    "    lemmatized_sentence = lemmatize_sentence(sentence)\n",
    "    lemmatized_sentences.append(lemmatized_sentence)\n",
    "\n",
    "corpus = ' '.join(lemmatized_sentences)\n",
    "corpus = corpus.lower()\n",
    "words = nltk.word_tokenize(corpus)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "final_tokens = []\n",
    "for each in words:\n",
    " if each not in stop_words:\n",
    "    final_tokens.append(each)\n",
    "final_tokens\n",
    "corpus = ' '.join(final_tokens)\n",
    "corpus = corpus.replace(\" . \", \" \")\n",
    "corpus = [corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "import nltk\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ngrams(sequence, n):\n",
    "    return list(\n",
    "            zip(*(sequence[index:] \n",
    "                     for index in range(n)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 3), (3, 4)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_ngrams([1,2,3,4], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3), (2, 3, 4)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_ngrams([1,2,3,4], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_corpus(corpus):\n",
    "    return ' '.join([document.strip() \n",
    "                     for document in corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_ngrams(corpus, ngram_val=1, limit=5):\n",
    "\n",
    "    corpus = flatten_corpus(corpus)\n",
    "    tokens = nltk.word_tokenize(corpus)\n",
    "\n",
    "    ngrams = compute_ngrams(tokens, ngram_val)\n",
    "    ngrams_freq_dist = nltk.FreqDist(ngrams)\n",
    "    sorted_ngrams_fd = sorted(ngrams_freq_dist.items(), \n",
    "                              key=itemgetter(1), reverse=True)\n",
    "    sorted_ngrams = sorted_ngrams_fd[0:limit]\n",
    "    sorted_ngrams = [(' '.join(text), freq) \n",
    "                     for text, freq in sorted_ngrams]\n",
    "\n",
    "    return sorted_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('potassium channel', 4),\n",
       " ('reward system', 3),\n",
       " ('treatment depression', 2),\n",
       " ('kcnq potassium', 2),\n",
       " ('channel within', 2),\n",
       " ('susceptible mouse', 2),\n",
       " ('within reward', 2),\n",
       " ('reward circuit', 2),\n",
       " ('depressive behavior', 2),\n",
       " ('patient mdd', 2)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_ngrams(corpus = corpus, ngram_val = 2,\n",
    "               limit = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kcnq potassium channel', 2),\n",
       " ('within reward circuit', 2),\n",
       " ('rest state functional', 2),\n",
       " ('depression lead cause', 1),\n",
       " ('lead cause disability', 1),\n",
       " ('cause disability worldwide', 1),\n",
       " ('disability worldwide available', 1),\n",
       " ('worldwide available treatment', 1),\n",
       " ('available treatment however', 1),\n",
       " ('treatment however partially', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_ngrams(corpus = corpus, ngram_val = 3,\n",
    "               limit = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams and Pointwise Mututal Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.collocations.BigramCollocationFinder at 0x1eac50a3d00>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import BigramAssocMeasures\n",
    "\n",
    "finder = BigramCollocationFinder.from_documents([item.split() \n",
    "                                                for item \n",
    "                                                in corpus])\n",
    "finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('potassium', 'channel'),\n",
       " ('reward', 'system'),\n",
       " ('brain', 'circuitry'),\n",
       " ('channel', 'within'),\n",
       " ('depressive', 'behavior'),\n",
       " ('kcnq', 'potassium'),\n",
       " ('measure', 'use'),\n",
       " ('patient', 'mdd'),\n",
       " ('rest', 'state'),\n",
       " ('reward', 'circuit')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = BigramAssocMeasures()                                                \n",
    "finder.nbest(bigram_measures.raw_freq, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('able', 'actively'),\n",
       " ('actively', 'counteract'),\n",
       " ('addition', 'clinical'),\n",
       " ('additional', 'limitation'),\n",
       " ('additionally', 'normalize'),\n",
       " ('administration', 'fda'),\n",
       " ('agent', 'largely'),\n",
       " ('antagonist', 'ketamine'),\n",
       " ('aspartate', 'receptor'),\n",
       " ('assess', 'specificity')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.nbest(bigram_measures.pmi, 10)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.collocations import TrigramAssocMeasures\n",
    "\n",
    "finder = TrigramCollocationFinder.from_documents([item.split() \n",
    "                                                for item \n",
    "                                                in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kcnq', 'potassium', 'channel'),\n",
       " ('rest', 'state', 'functional'),\n",
       " ('within', 'reward', 'circuit'),\n",
       " ('able', 'actively', 'counteract'),\n",
       " ('accumbens', 'nac', 'within'),\n",
       " ('action', 'base', 'decade'),\n",
       " ('action', 'undesirable', 'side'),\n",
       " ('active', 'mechanism', 'susceptible'),\n",
       " ('actively', 'counteract', 'vta'),\n",
       " ('activity', 'kcnq', 'channel')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_measures = TrigramAssocMeasures()                                                \n",
    "finder.nbest(trigram_measures.raw_freq, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('able', 'actively', 'counteract'),\n",
       " ('addition', 'clinical', 'anhedonia'),\n",
       " ('additional', 'limitation', 'include'),\n",
       " ('administration', 'fda', 'approve'),\n",
       " ('agent', 'largely', 'share'),\n",
       " ('antagonist', 'ketamine', 'separate'),\n",
       " ('aspartate', 'receptor', 'antagonist'),\n",
       " ('cause', 'disability', 'worldwide'),\n",
       " ('cd', 'risc', 'hypothesize'),\n",
       " ('consist', 'serotonergic', 'noradrenergic')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.nbest(trigram_measures.pmi, 10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Tag-based Phrase Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Depression is a leading cause of disability worldwide.',\n",
       " 'Available treatments however are only partially effective for many patients and are associated with additional limitations including a slow onset of therapeutic action and undesirable side effects.',\n",
       " 'Currently the Food and Drug Administration FDA approved treatments for depression mostly consisting of serotonergic and noradrenergic agents largely share the same basic pharmacology and mechanism of action based on decades old discoveries.',\n",
       " 'This lack of mechanistic diversity leaves little opportunity for improved patient outcomes or personalized treatment approaches.',\n",
       " 'In contrast rational drug discovery based on a mechanistic understanding of disease pathology promises to deliver more effective targeted therapies.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def get_chunks(sentences, grammar = r'NP: {<DT>? <JJ>* <NN.*>+}', stopword_list=stopwords):\n",
    "    \n",
    "    all_chunks = []\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        \n",
    "        tagged_sents = [nltk.pos_tag(nltk.word_tokenize(sentence))]   \n",
    "        \n",
    "        chunks = [chunker.parse(tagged_sent) \n",
    "                      for tagged_sent in tagged_sents]\n",
    "        \n",
    "        wtc_sents = [nltk.chunk.tree2conlltags(chunk)\n",
    "                         for chunk in chunks]    \n",
    "        \n",
    "        flattened_chunks = list(\n",
    "                            itertools.chain.from_iterable(\n",
    "                                wtc_sent for wtc_sent in wtc_sents)\n",
    "                           )\n",
    "        \n",
    "        valid_chunks_tagged = [(status, [wtc for wtc in chunk]) \n",
    "                                   for status, chunk \n",
    "                                       in itertools.groupby(flattened_chunks, \n",
    "                                                lambda word_pos_chunk: word_pos_chunk[2] != 'O')]\n",
    "        \n",
    "        valid_chunks = [' '.join(word.lower() \n",
    "                                for word, tag, chunk in wtc_group \n",
    "                                    if word.lower() not in stopword_list) \n",
    "                                        for status, wtc_group in valid_chunks_tagged\n",
    "                                            if status]\n",
    "                                            \n",
    "        all_chunks.append(valid_chunks)\n",
    "    \n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['depression', 'cause', 'disability worldwide'],\n",
       " ['available treatments',\n",
       "  'many patients',\n",
       "  'additional limitations',\n",
       "  'slow onset',\n",
       "  'therapeutic action',\n",
       "  'undesirable side effects']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = get_chunks(sentences)\n",
    "chunks[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "def get_tfidf_weighted_keyphrases(sentences, \n",
    "                                  grammar=r'NP: {<DT>? <JJ>* <NN.*>+}',\n",
    "                                  top_n=10):\n",
    "    valid_chunks = get_chunks(sentences, grammar=grammar)                          \n",
    "    dictionary = corpora.Dictionary(valid_chunks)\n",
    "    corpus = [dictionary.doc2bow(chunk) for chunk in valid_chunks]\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    weighted_phrases = {dictionary.get(idx): value \n",
    "                           for doc in corpus_tfidf \n",
    "                               for idx, value in doc}                \n",
    "    weighted_phrases = sorted(weighted_phrases.items(), \n",
    "                              key=itemgetter(1), reverse=True)\n",
    "    weighted_phrases = [(term, round(wt, 3)) for term, wt in weighted_phrases]\n",
    "    return weighted_phrases[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('resilience', 0.747),\n",
       " ('cause', 0.646),\n",
       " ('disability worldwide', 0.646),\n",
       " ('dopaminergic firing', 0.58),\n",
       " ('vta nac hyperactivity', 0.58),\n",
       " ('circuit', 0.577),\n",
       " ('dysregulation', 0.577),\n",
       " ('resilient animals', 0.577),\n",
       " ('contrast rational drug discovery', 0.5),\n",
       " ('disease pathology', 0.5)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tfidf_weighted_keyphrases(sentences = sentences, top_n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reward', 0.174),\n",
       " ('function strengthen resilience', 0.122),\n",
       " ('antidepressant', 0.116),\n",
       " ('patient associate', 0.103),\n",
       " ('rodent express pro depressive', 0.092),\n",
       " ('mdd connectivity', 0.083),\n",
       " ('distinct phenotype susceptible', 0.078),\n",
       " ('translational importance systemic injection ezogabine', 0.077),\n",
       " ('change', 0.076),\n",
       " ('validate mouse model', 0.076)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.summarization import keywords\n",
    "key_words = keywords(corpus[0], ratio=1.0, scores=True, lemmatize=True)\n",
    "[(item, round(score, 3)) for item, score in key_words][:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
